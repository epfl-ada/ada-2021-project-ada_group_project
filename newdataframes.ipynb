{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc55523-27d4-4c84-9973-4cc04be00310",
   "metadata": {},
   "source": [
    "## Creation of the quote dataframes for each year --updated (output is in google drive folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570b0f88-57e2-4cfb-968f-c20fb042340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83758c28-4b34-4e56-b408-95178728e067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>aliases</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>nationality</th>\n",
       "      <th>gender</th>\n",
       "      <th>lastrevid</th>\n",
       "      <th>ethnic_group</th>\n",
       "      <th>US_congress_bio_ID</th>\n",
       "      <th>occupation</th>\n",
       "      <th>party</th>\n",
       "      <th>academic_degree</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>candidacy</th>\n",
       "      <th>type</th>\n",
       "      <th>religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['George Walker Bush' 'Bush Jr.' 'Dubya' 'GWB'...</td>\n",
       "      <td>['+1946-07-06T00:00:00Z']</td>\n",
       "      <td>['Q30']</td>\n",
       "      <td>['Q6581097']</td>\n",
       "      <td>1395142029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Q82955' 'Q15982858' 'Q18814623' 'Q1028181' '...</td>\n",
       "      <td>['Q29468']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q207</td>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>['Q327959' 'Q464075' 'Q3586276' 'Q4450587']</td>\n",
       "      <td>item</td>\n",
       "      <td>['Q329646' 'Q682443' 'Q33203']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['Willard Mitt Romney' 'Pierre Delecto']</td>\n",
       "      <td>['+1947-03-12T00:00:00Z']</td>\n",
       "      <td>['Q30']</td>\n",
       "      <td>['Q6581097']</td>\n",
       "      <td>1393565531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R000615</td>\n",
       "      <td>['Q82955' 'Q15978655' 'Q43845' 'Q15980158' 'Q2...</td>\n",
       "      <td>['Q29468']</td>\n",
       "      <td>['Q1765120' 'Q191701' 'Q1540185']</td>\n",
       "      <td>Q4496</td>\n",
       "      <td>Mitt Romney</td>\n",
       "      <td>['Q937607' 'Q4226' 'Q4791860' 'Q17100322']</td>\n",
       "      <td>item</td>\n",
       "      <td>['Q42504']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['Stephen Gerald Breyer']</td>\n",
       "      <td>['+1938-08-15T00:00:00Z']</td>\n",
       "      <td>['Q30']</td>\n",
       "      <td>['Q6581097']</td>\n",
       "      <td>1393110898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Q185351' 'Q16533' 'Q40348' 'Q1622272' 'Q82955']</td>\n",
       "      <td>['Q29552']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q11124</td>\n",
       "      <td>Stephen Breyer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>item</td>\n",
       "      <td>['Q9268']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>['David Alexander Paterson']</td>\n",
       "      <td>['+1954-05-20T00:00:00Z']</td>\n",
       "      <td>['Q30']</td>\n",
       "      <td>['Q6581097']</td>\n",
       "      <td>1392736069</td>\n",
       "      <td>['Q49085']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Q82955']</td>\n",
       "      <td>['Q29552']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q11674</td>\n",
       "      <td>David Paterson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>item</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>['James Warren \"Jim\" DeMint' 'James Warren DeM...</td>\n",
       "      <td>['+1951-09-02T00:00:00Z']</td>\n",
       "      <td>['Q30']</td>\n",
       "      <td>['Q6581097']</td>\n",
       "      <td>1392634298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D000595</td>\n",
       "      <td>['Q82955' 'Q2961975']</td>\n",
       "      <td>['Q29468']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q22201</td>\n",
       "      <td>Jim DeMint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>item</td>\n",
       "      <td>['Q178169']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            aliases  \\\n",
       "0           0  ['George Walker Bush' 'Bush Jr.' 'Dubya' 'GWB'...   \n",
       "1           1           ['Willard Mitt Romney' 'Pierre Delecto']   \n",
       "2           2                          ['Stephen Gerald Breyer']   \n",
       "3           3                       ['David Alexander Paterson']   \n",
       "4           4  ['James Warren \"Jim\" DeMint' 'James Warren DeM...   \n",
       "\n",
       "               date_of_birth nationality        gender   lastrevid  \\\n",
       "0  ['+1946-07-06T00:00:00Z']     ['Q30']  ['Q6581097']  1395142029   \n",
       "1  ['+1947-03-12T00:00:00Z']     ['Q30']  ['Q6581097']  1393565531   \n",
       "2  ['+1938-08-15T00:00:00Z']     ['Q30']  ['Q6581097']  1393110898   \n",
       "3  ['+1954-05-20T00:00:00Z']     ['Q30']  ['Q6581097']  1392736069   \n",
       "4  ['+1951-09-02T00:00:00Z']     ['Q30']  ['Q6581097']  1392634298   \n",
       "\n",
       "  ethnic_group US_congress_bio_ID  \\\n",
       "0          NaN                NaN   \n",
       "1          NaN            R000615   \n",
       "2          NaN                NaN   \n",
       "3   ['Q49085']                NaN   \n",
       "4          NaN            D000595   \n",
       "\n",
       "                                          occupation       party  \\\n",
       "0  ['Q82955' 'Q15982858' 'Q18814623' 'Q1028181' '...  ['Q29468']   \n",
       "1  ['Q82955' 'Q15978655' 'Q43845' 'Q15980158' 'Q2...  ['Q29468']   \n",
       "2  ['Q185351' 'Q16533' 'Q40348' 'Q1622272' 'Q82955']  ['Q29552']   \n",
       "3                                         ['Q82955']  ['Q29552']   \n",
       "4                              ['Q82955' 'Q2961975']  ['Q29468']   \n",
       "\n",
       "                     academic_degree      id           label  \\\n",
       "0                                NaN    Q207  George W. Bush   \n",
       "1  ['Q1765120' 'Q191701' 'Q1540185']   Q4496     Mitt Romney   \n",
       "2                                NaN  Q11124  Stephen Breyer   \n",
       "3                                NaN  Q11674  David Paterson   \n",
       "4                                NaN  Q22201      Jim DeMint   \n",
       "\n",
       "                                     candidacy  type  \\\n",
       "0  ['Q327959' 'Q464075' 'Q3586276' 'Q4450587']  item   \n",
       "1   ['Q937607' 'Q4226' 'Q4791860' 'Q17100322']  item   \n",
       "2                                          NaN  item   \n",
       "3                                          NaN  item   \n",
       "4                                          NaN  item   \n",
       "\n",
       "                         religion  \n",
       "0  ['Q329646' 'Q682443' 'Q33203']  \n",
       "1                      ['Q42504']  \n",
       "2                       ['Q9268']  \n",
       "3                             NaN  \n",
       "4                     ['Q178169']  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "american_politicians = pd.read_csv(\"data/american_politicians.csv\")\n",
    "american_politicians.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a143aed3-b8e5-4903-aef1-0ae49e0cb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = american_politicians.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b5f2b9-dbef-499b-b417-dd8d587dc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['lesbian', 'gay', 'homosexual', 'gender', 'bisexual', 'sexuality', 'same sex', 'ally',\n",
    "         'asexual', 'bi', 'biphobia', 'bisexual', 'coming out', 'coming-out', 'gender identity',\n",
    "        'queer', 'genderqueer', 'gender-queer', 'homophobia', 'LGBTQ', 'LGBT', 'LGBTQ+', 'LGBTQIA',\n",
    "        'lgbtq', 'lgbt', 'lgbtq+', 'lgbtqia', 'non binary', 'non-binary', 'transgender'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b2b207-f7b5-431e-87f2-1ff15bb76a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 28451 rows\n"
     ]
    }
   ],
   "source": [
    "chunk_num = 1\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        \n",
    "        return chunk.loc[chunk.speaker.isin(names) & (chunk.quotation.str.contains('{}'.format(\"|\".join(words))))]\n",
    "            \n",
    "        \n",
    "\n",
    "with pd.read_json('../Downloads/quotes-2018.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe18 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe18 = pd.concat([dataframe18, processed_chunk])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a33c2f89-8f09-40a3-b037-d94f2f50087d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326946, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe18.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d325c89-ca08-4f80-b451-a1669cdf9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe18.to_csv(\"data/df18.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3545e4c1-da38-433f-ac4d-2717c02e1b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 11588 rows\n"
     ]
    }
   ],
   "source": [
    "chunk_num = 1\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        \n",
    "        return chunk.loc[chunk.speaker.isin(names) & (chunk.quotation.str.contains('{}'.format(\"|\".join(words))))]\n",
    "            \n",
    "        \n",
    "\n",
    "with pd.read_json('../Downloads/quotes-2017.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe17 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe17 = pd.concat([dataframe17, processed_chunk])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e888d407-8553-4475-90d9-761256102e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341425, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daecc0bb-c3aa-4476-9e77-3d5e6caca300",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe17.to_csv(\"data/df17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eed11fe-5c18-4109-9f6c-ac1620c7bab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 74338 rows\n"
     ]
    }
   ],
   "source": [
    "chunk_num = 1\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        \n",
    "        return chunk.loc[chunk.speaker.isin(names) & (chunk.quotation.str.contains('{}'.format(\"|\".join(words))))]\n",
    "            \n",
    "        \n",
    "\n",
    "with pd.read_json('../Downloads/quotes-2015.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe15 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe15 = pd.concat([dataframe15, processed_chunk])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08c82a9e-8f42-43bd-a61d-0e45833f4f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2015-09-08-000937</td>\n",
       "      <td>365 degrees is above the glass transition phas...</td>\n",
       "      <td>Tim Moore</td>\n",
       "      <td>[Q16123784, Q16732824, Q7804002, Q7804003, Q78...</td>\n",
       "      <td>2015-09-08 12:47:51</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Tim Moore, 0.8868], [None, 0.1132]]</td>\n",
       "      <td>[http://www.thefashionspot.com/beauty/637227-i...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2015-04-17-024619</td>\n",
       "      <td>I am hopeful that there is room to address the...</td>\n",
       "      <td>Kirk Watson</td>\n",
       "      <td>[Q6415545, Q6415546]</td>\n",
       "      <td>2015-04-17 18:17:08</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Kirk Watson, 0.8465], [None, 0.1347], [Sylve...</td>\n",
       "      <td>[http://govtech.com/state/Texas-Lawmakers-Deba...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2015-08-04-030016</td>\n",
       "      <td>I believe there is a reasonable way to address...</td>\n",
       "      <td>Marco Rubio</td>\n",
       "      <td>[Q324546]</td>\n",
       "      <td>2015-08-04 13:19:27</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Marco Rubio, 0.8876], [None, 0.0669], [Jeff ...</td>\n",
       "      <td>[http://breitbart.com/big-government/2015/08/0...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>2015-08-24-027337</td>\n",
       "      <td>I congratulate Joel and Andrew Melamed for rec...</td>\n",
       "      <td>Michelle Schimel</td>\n",
       "      <td>[Q6837234]</td>\n",
       "      <td>2015-08-24 12:06:10</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Michelle Schimel, 0.6288], [None, 0.2246], [...</td>\n",
       "      <td>[http://www.longisland.com/news/08-22-15/alber...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>2015-10-12-002961</td>\n",
       "      <td>actively promote childbirth instead of abortion.</td>\n",
       "      <td>Mike Pence</td>\n",
       "      <td>[Q24313]</td>\n",
       "      <td>2015-10-12 22:58:50</td>\n",
       "      <td>12</td>\n",
       "      <td>[[Mike Pence, 0.5939], [None, 0.4003], [Donald...</td>\n",
       "      <td>[http://theindychannel.com/news/local-news/ant...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20873751</th>\n",
       "      <td>2015-03-04-018543</td>\n",
       "      <td>For the sake of those people who are taking ca...</td>\n",
       "      <td>Jessyn Farrell</td>\n",
       "      <td>[Q6187944]</td>\n",
       "      <td>2015-03-04 01:57:31</td>\n",
       "      <td>3</td>\n",
       "      <td>[[Jessyn Farrell, 0.8084], [None, 0.115], [Liz...</td>\n",
       "      <td>[http://www.komonews.com/news/local/12-minimum...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20873931</th>\n",
       "      <td>2015-02-04-020562</td>\n",
       "      <td>has signed two big pieces of pro-life legislat...</td>\n",
       "      <td>Nikki Haley</td>\n",
       "      <td>[Q11668]</td>\n",
       "      <td>2015-02-04 20:28:16</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Nikki Haley, 0.9043], [None, 0.0873], [Dan M...</td>\n",
       "      <td>[http://ncronline.org/news/politics/pro-life-a...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20874015</th>\n",
       "      <td>2015-10-17-014629</td>\n",
       "      <td>He [ Hansen ] was writing that grant while he ...</td>\n",
       "      <td>Teresa Fedor</td>\n",
       "      <td>[Q7702134]</td>\n",
       "      <td>2015-10-17 04:53:12</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Teresa Fedor, 0.7461], [None, 0.1156], [Rich...</td>\n",
       "      <td>[http://toledoblade.com/MarilouJohanek/2015/10...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20874082</th>\n",
       "      <td>2015-04-18-009670</td>\n",
       "      <td>He played unbelievable tonight. He came up and...</td>\n",
       "      <td>Mike Green</td>\n",
       "      <td>[Q1381562, Q16123603, Q1612734, Q16729427, Q19...</td>\n",
       "      <td>2015-04-18 04:54:35</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Mike Green, 0.937], [None, 0.0586], [Philipp...</td>\n",
       "      <td>[http://feeds.washingtonpost.com/c/34656/f/636...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20874327</th>\n",
       "      <td>2015-01-14-023293</td>\n",
       "      <td>Hopefully we'll get to a point that the counci...</td>\n",
       "      <td>Rory Lancman</td>\n",
       "      <td>[Q7366901]</td>\n",
       "      <td>2015-01-14 14:23:50</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Rory Lancman, 0.9398], [None, 0.0495], [Bill...</td>\n",
       "      <td>[http://adn.com/article/20150114/mayor-vows-ve...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268343 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    quoteID  \\\n",
       "98        2015-09-08-000937   \n",
       "167       2015-04-17-024619   \n",
       "273       2015-08-04-030016   \n",
       "375       2015-08-24-027337   \n",
       "482       2015-10-12-002961   \n",
       "...                     ...   \n",
       "20873751  2015-03-04-018543   \n",
       "20873931  2015-02-04-020562   \n",
       "20874015  2015-10-17-014629   \n",
       "20874082  2015-04-18-009670   \n",
       "20874327  2015-01-14-023293   \n",
       "\n",
       "                                                  quotation           speaker  \\\n",
       "98        365 degrees is above the glass transition phas...         Tim Moore   \n",
       "167       I am hopeful that there is room to address the...       Kirk Watson   \n",
       "273       I believe there is a reasonable way to address...       Marco Rubio   \n",
       "375       I congratulate Joel and Andrew Melamed for rec...  Michelle Schimel   \n",
       "482        actively promote childbirth instead of abortion.        Mike Pence   \n",
       "...                                                     ...               ...   \n",
       "20873751  For the sake of those people who are taking ca...    Jessyn Farrell   \n",
       "20873931  has signed two big pieces of pro-life legislat...       Nikki Haley   \n",
       "20874015  He [ Hansen ] was writing that grant while he ...      Teresa Fedor   \n",
       "20874082  He played unbelievable tonight. He came up and...        Mike Green   \n",
       "20874327  Hopefully we'll get to a point that the counci...      Rory Lancman   \n",
       "\n",
       "                                                       qids  \\\n",
       "98        [Q16123784, Q16732824, Q7804002, Q7804003, Q78...   \n",
       "167                                    [Q6415545, Q6415546]   \n",
       "273                                               [Q324546]   \n",
       "375                                              [Q6837234]   \n",
       "482                                                [Q24313]   \n",
       "...                                                     ...   \n",
       "20873751                                         [Q6187944]   \n",
       "20873931                                           [Q11668]   \n",
       "20874015                                         [Q7702134]   \n",
       "20874082  [Q1381562, Q16123603, Q1612734, Q16729427, Q19...   \n",
       "20874327                                         [Q7366901]   \n",
       "\n",
       "                        date  numOccurrences  \\\n",
       "98       2015-09-08 12:47:51               1   \n",
       "167      2015-04-17 18:17:08               1   \n",
       "273      2015-08-04 13:19:27               1   \n",
       "375      2015-08-24 12:06:10               2   \n",
       "482      2015-10-12 22:58:50              12   \n",
       "...                      ...             ...   \n",
       "20873751 2015-03-04 01:57:31               3   \n",
       "20873931 2015-02-04 20:28:16               1   \n",
       "20874015 2015-10-17 04:53:12               1   \n",
       "20874082 2015-04-18 04:54:35               2   \n",
       "20874327 2015-01-14 14:23:50               1   \n",
       "\n",
       "                                                     probas  \\\n",
       "98                    [[Tim Moore, 0.8868], [None, 0.1132]]   \n",
       "167       [[Kirk Watson, 0.8465], [None, 0.1347], [Sylve...   \n",
       "273       [[Marco Rubio, 0.8876], [None, 0.0669], [Jeff ...   \n",
       "375       [[Michelle Schimel, 0.6288], [None, 0.2246], [...   \n",
       "482       [[Mike Pence, 0.5939], [None, 0.4003], [Donald...   \n",
       "...                                                     ...   \n",
       "20873751  [[Jessyn Farrell, 0.8084], [None, 0.115], [Liz...   \n",
       "20873931  [[Nikki Haley, 0.9043], [None, 0.0873], [Dan M...   \n",
       "20874015  [[Teresa Fedor, 0.7461], [None, 0.1156], [Rich...   \n",
       "20874082  [[Mike Green, 0.937], [None, 0.0586], [Philipp...   \n",
       "20874327  [[Rory Lancman, 0.9398], [None, 0.0495], [Bill...   \n",
       "\n",
       "                                                       urls phase  \n",
       "98        [http://www.thefashionspot.com/beauty/637227-i...     E  \n",
       "167       [http://govtech.com/state/Texas-Lawmakers-Deba...     E  \n",
       "273       [http://breitbart.com/big-government/2015/08/0...     E  \n",
       "375       [http://www.longisland.com/news/08-22-15/alber...     E  \n",
       "482       [http://theindychannel.com/news/local-news/ant...     E  \n",
       "...                                                     ...   ...  \n",
       "20873751  [http://www.komonews.com/news/local/12-minimum...     E  \n",
       "20873931  [http://ncronline.org/news/politics/pro-life-a...     E  \n",
       "20874015  [http://toledoblade.com/MarilouJohanek/2015/10...     E  \n",
       "20874082  [http://feeds.washingtonpost.com/c/34656/f/636...     E  \n",
       "20874327  [http://adn.com/article/20150114/mayor-vows-ve...     E  \n",
       "\n",
       "[268343 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fb35735-0fb5-41fc-9e4c-0f63c6b26e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268343, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe15.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "329a8f16-cf2f-4fb9-8bcb-52c60659813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe15.to_csv(\"data/df15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4d9a4f3-e3fc-4a37-9363-2295497b083f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98          365 degrees is above the glass transition phas...\n",
       "167         I am hopeful that there is room to address the...\n",
       "273         I believe there is a reasonable way to address...\n",
       "375         I congratulate Joel and Andrew Melamed for rec...\n",
       "482          actively promote childbirth instead of abortion.\n",
       "                                  ...                        \n",
       "20873751    For the sake of those people who are taking ca...\n",
       "20873931    has signed two big pieces of pro-life legislat...\n",
       "20874015    He [ Hansen ] was writing that grant while he ...\n",
       "20874082    He played unbelievable tonight. He came up and...\n",
       "20874327    Hopefully we'll get to a point that the counci...\n",
       "Name: quotation, Length: 268343, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe15.quotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef27aad-3428-49f4-97b5-137d662859c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 44449 rows\n"
     ]
    }
   ],
   "source": [
    "chunk_num = 1\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        \n",
    "        return chunk.loc[chunk.speaker.isin(names) & (chunk.quotation.str.contains('{}'.format(\"|\".join(words))))]\n",
    "            \n",
    "        \n",
    "\n",
    "with pd.read_json('../Downloads/quotes-2020.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe20 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe20 = pd.concat([dataframe20, processed_chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8697786-dd6e-4eef-be10-aafbf9bb7c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71692, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd10d898-0581-4061-8d9a-5fc0dab522b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe20.to_csv(\"data/df20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d1cc178-1548-4c04-a85d-c7f725b9ffa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2015-09-08-000937</td>\n",
       "      <td>365 degrees is above the glass transition phas...</td>\n",
       "      <td>Tim Moore</td>\n",
       "      <td>[Q16123784, Q16732824, Q7804002, Q7804003, Q78...</td>\n",
       "      <td>2015-09-08 12:47:51</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Tim Moore, 0.8868], [None, 0.1132]]</td>\n",
       "      <td>[http://www.thefashionspot.com/beauty/637227-i...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2015-04-17-024619</td>\n",
       "      <td>I am hopeful that there is room to address the...</td>\n",
       "      <td>Kirk Watson</td>\n",
       "      <td>[Q6415545, Q6415546]</td>\n",
       "      <td>2015-04-17 18:17:08</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Kirk Watson, 0.8465], [None, 0.1347], [Sylve...</td>\n",
       "      <td>[http://govtech.com/state/Texas-Lawmakers-Deba...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2015-08-04-030016</td>\n",
       "      <td>I believe there is a reasonable way to address...</td>\n",
       "      <td>Marco Rubio</td>\n",
       "      <td>[Q324546]</td>\n",
       "      <td>2015-08-04 13:19:27</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Marco Rubio, 0.8876], [None, 0.0669], [Jeff ...</td>\n",
       "      <td>[http://breitbart.com/big-government/2015/08/0...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>2015-08-24-027337</td>\n",
       "      <td>I congratulate Joel and Andrew Melamed for rec...</td>\n",
       "      <td>Michelle Schimel</td>\n",
       "      <td>[Q6837234]</td>\n",
       "      <td>2015-08-24 12:06:10</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Michelle Schimel, 0.6288], [None, 0.2246], [...</td>\n",
       "      <td>[http://www.longisland.com/news/08-22-15/alber...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>2015-10-12-002961</td>\n",
       "      <td>actively promote childbirth instead of abortion.</td>\n",
       "      <td>Mike Pence</td>\n",
       "      <td>[Q24313]</td>\n",
       "      <td>2015-10-12 22:58:50</td>\n",
       "      <td>12</td>\n",
       "      <td>[[Mike Pence, 0.5939], [None, 0.4003], [Donald...</td>\n",
       "      <td>[http://theindychannel.com/news/local-news/ant...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5244280</th>\n",
       "      <td>2020-01-10-101698</td>\n",
       "      <td>We're really thankful he didn't get hit by that.</td>\n",
       "      <td>Rick Johnson</td>\n",
       "      <td>[Q16220978, Q16730371, Q7331487, Q7331488, Q73...</td>\n",
       "      <td>2020-01-10 06:55:32</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Rick Johnson, 0.9574], [None, 0.0426]]</td>\n",
       "      <td>[https://www.seattletimes.com/seattle-news/man...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5244340</th>\n",
       "      <td>2020-02-20-093793</td>\n",
       "      <td>When they want a bill, they bring the bill to ...</td>\n",
       "      <td>J.T. Wilcox</td>\n",
       "      <td>[Q6104393]</td>\n",
       "      <td>2020-02-20 22:12:45</td>\n",
       "      <td>1</td>\n",
       "      <td>[[J.T. Wilcox, 0.5884], [None, 0.3877], [Jay I...</td>\n",
       "      <td>[http://www.spokesman.com/stories/2020/feb/21/...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5244371</th>\n",
       "      <td>2020-02-05-121101</td>\n",
       "      <td>Why would the Commonwealth allow for the expan...</td>\n",
       "      <td>Tom Wolf</td>\n",
       "      <td>[Q1557544, Q7794795]</td>\n",
       "      <td>2020-02-05 01:35:43</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Tom Wolf, 0.9219], [None, 0.0717], [Presiden...</td>\n",
       "      <td>[https://heavy.com/news/2020/02/janiyah-davis-...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5244407</th>\n",
       "      <td>2020-04-07-071722</td>\n",
       "      <td>You can come to a polling place and do it safe...</td>\n",
       "      <td>Robin Vos</td>\n",
       "      <td>[Q7352841]</td>\n",
       "      <td>2020-04-07 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>[[Robin Vos, 0.7618], [None, 0.2189], [Tony Ev...</td>\n",
       "      <td>[http://uk.businessinsider.com/wisconsin-gop-l...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5244422</th>\n",
       "      <td>2020-02-20-097170</td>\n",
       "      <td>You have to accept some responsibility and ask...</td>\n",
       "      <td>Pete Buttigieg</td>\n",
       "      <td>[Q7173106]</td>\n",
       "      <td>2020-02-20 03:50:07</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Pete Buttigieg, 0.3517], [Bernie Sanders, 0....</td>\n",
       "      <td>[https://www.breitbart.com/2020-election/2020/...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268957 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   quoteID                                          quotation  \\\n",
       "98       2015-09-08-000937  365 degrees is above the glass transition phas...   \n",
       "167      2015-04-17-024619  I am hopeful that there is room to address the...   \n",
       "273      2015-08-04-030016  I believe there is a reasonable way to address...   \n",
       "375      2015-08-24-027337  I congratulate Joel and Andrew Melamed for rec...   \n",
       "482      2015-10-12-002961   actively promote childbirth instead of abortion.   \n",
       "...                    ...                                                ...   \n",
       "5244280  2020-01-10-101698   We're really thankful he didn't get hit by that.   \n",
       "5244340  2020-02-20-093793  When they want a bill, they bring the bill to ...   \n",
       "5244371  2020-02-05-121101  Why would the Commonwealth allow for the expan...   \n",
       "5244407  2020-04-07-071722  You can come to a polling place and do it safe...   \n",
       "5244422  2020-02-20-097170  You have to accept some responsibility and ask...   \n",
       "\n",
       "                  speaker                                               qids  \\\n",
       "98              Tim Moore  [Q16123784, Q16732824, Q7804002, Q7804003, Q78...   \n",
       "167           Kirk Watson                               [Q6415545, Q6415546]   \n",
       "273           Marco Rubio                                          [Q324546]   \n",
       "375      Michelle Schimel                                         [Q6837234]   \n",
       "482            Mike Pence                                           [Q24313]   \n",
       "...                   ...                                                ...   \n",
       "5244280      Rick Johnson  [Q16220978, Q16730371, Q7331487, Q7331488, Q73...   \n",
       "5244340       J.T. Wilcox                                         [Q6104393]   \n",
       "5244371          Tom Wolf                               [Q1557544, Q7794795]   \n",
       "5244407         Robin Vos                                         [Q7352841]   \n",
       "5244422    Pete Buttigieg                                         [Q7173106]   \n",
       "\n",
       "                       date  numOccurrences  \\\n",
       "98      2015-09-08 12:47:51               1   \n",
       "167     2015-04-17 18:17:08               1   \n",
       "273     2015-08-04 13:19:27               1   \n",
       "375     2015-08-24 12:06:10               2   \n",
       "482     2015-10-12 22:58:50              12   \n",
       "...                     ...             ...   \n",
       "5244280 2020-01-10 06:55:32               1   \n",
       "5244340 2020-02-20 22:12:45               1   \n",
       "5244371 2020-02-05 01:35:43               1   \n",
       "5244407 2020-04-07 00:00:00               3   \n",
       "5244422 2020-02-20 03:50:07               2   \n",
       "\n",
       "                                                    probas  \\\n",
       "98                   [[Tim Moore, 0.8868], [None, 0.1132]]   \n",
       "167      [[Kirk Watson, 0.8465], [None, 0.1347], [Sylve...   \n",
       "273      [[Marco Rubio, 0.8876], [None, 0.0669], [Jeff ...   \n",
       "375      [[Michelle Schimel, 0.6288], [None, 0.2246], [...   \n",
       "482      [[Mike Pence, 0.5939], [None, 0.4003], [Donald...   \n",
       "...                                                    ...   \n",
       "5244280           [[Rick Johnson, 0.9574], [None, 0.0426]]   \n",
       "5244340  [[J.T. Wilcox, 0.5884], [None, 0.3877], [Jay I...   \n",
       "5244371  [[Tom Wolf, 0.9219], [None, 0.0717], [Presiden...   \n",
       "5244407  [[Robin Vos, 0.7618], [None, 0.2189], [Tony Ev...   \n",
       "5244422  [[Pete Buttigieg, 0.3517], [Bernie Sanders, 0....   \n",
       "\n",
       "                                                      urls phase  \n",
       "98       [http://www.thefashionspot.com/beauty/637227-i...     E  \n",
       "167      [http://govtech.com/state/Texas-Lawmakers-Deba...     E  \n",
       "273      [http://breitbart.com/big-government/2015/08/0...     E  \n",
       "375      [http://www.longisland.com/news/08-22-15/alber...     E  \n",
       "482      [http://theindychannel.com/news/local-news/ant...     E  \n",
       "...                                                    ...   ...  \n",
       "5244280  [https://www.seattletimes.com/seattle-news/man...     E  \n",
       "5244340  [http://www.spokesman.com/stories/2020/feb/21/...     E  \n",
       "5244371  [https://heavy.com/news/2020/02/janiyah-davis-...     E  \n",
       "5244407  [http://uk.businessinsider.com/wisconsin-gop-l...     E  \n",
       "5244422  [https://www.breitbart.com/2020-election/2020/...     E  \n",
       "\n",
       "[268957 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c7431d-25c9-49eb-bd2a-639a1a451325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 63302 rows\n"
     ]
    }
   ],
   "source": [
    "chunk_num = 1\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        \n",
    "        return chunk.loc[chunk.speaker.isin(names) & (chunk.quotation.str.contains('{}'.format(\"|\".join(words))))]\n",
    "            \n",
    "        \n",
    "\n",
    "with pd.read_json('../Downloads/quotes-2019.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe19 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe19 = pd.concat([dataframe19, processed_chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94ebfafb-aca6-4834-b4bb-2d9d05f45cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268995, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c987553-6e06-4b90-aa05-2290d694fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe19.to_csv(\"data/df19.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9802c3-b968-47b3-8d6e-0e59acd47154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 62129 rows\n"
     ]
    }
   ],
   "source": [
    "chunk_num = 1\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        \n",
    "        return chunk.loc[chunk.speaker.isin(names) & (chunk.quotation.str.contains('{}'.format(\"|\".join(words))))]\n",
    "            \n",
    "        \n",
    "\n",
    "with pd.read_json('../Downloads/quotes-2016.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe16 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe16 = pd.concat([dataframe16, processed_chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db009477-3833-495f-bb92-87ba3a66be2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178314, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77a1f64d-db03-4753-add8-dc70508247b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe16.to_csv(\"data/df16.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a55a0-3126-4c2f-9258-a778cc437ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n"
     ]
    }
   ],
   "source": [
    "chunk_num = 1\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        \n",
    "        return chunk.loc[chunk.speaker.isin(names) & (chunk.quotation.str.contains('{}'.format(\"|\".join(words))))]\n",
    "            \n",
    "        \n",
    "\n",
    "with pd.read_json('../Downloads/quotes-2018.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe18 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe18 = pd.concat([dataframe18, processed_chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094fda8a-6893-4c9c-9e4d-7504d8f1f92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
