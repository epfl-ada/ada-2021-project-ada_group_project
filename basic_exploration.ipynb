{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10584bdc-bd5d-4da2-bfe8-8efa0090bb96",
   "metadata": {},
   "source": [
    "### Very Basic Extraction of Possibly Relevant Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc6ce83-5945-4e3a-b119-41cedf345ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "politicians = pd.read_csv(\"data/politicians.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca09924-f9b6-4fca-914b-bfcd6e8b219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = politicians.speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba013f77-4074-4e60-b7cb-7a4811d4f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['lesbian', 'gay', 'homosexual', 'gender', 'bisexual', 'sexuality', 'same sex'] # obviously add more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e748346-0d83-42d4-943f-86de9af863d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Richard Shelby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Tommy Tuberville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Lisa Murkowski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Dan Sullivan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Kyrsten Sinema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>Texas</td>\n",
       "      <td>Van Taylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>Texas</td>\n",
       "      <td>Veronica Escobar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Florida</td>\n",
       "      <td>W. Gregory Steube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>William R. Timmons IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>Xochitl Torres Small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>695 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              state                speaker\n",
       "0           Alabama         Richard Shelby\n",
       "1           Alabama       Tommy Tuberville\n",
       "2            Alaska         Lisa Murkowski\n",
       "3            Alaska           Dan Sullivan\n",
       "4           Arizona         Kyrsten Sinema\n",
       "..              ...                    ...\n",
       "690           Texas             Van Taylor\n",
       "691           Texas       Veronica Escobar\n",
       "692         Florida      W. Gregory Steube\n",
       "693  South Carolina  William R. Timmons IV\n",
       "694      New Mexico   Xochitl Torres Small\n",
       "\n",
       "[695 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6eb008-7a3f-4f32-94f3-1bec6d277a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 100000 rows\n",
      "Processing chunk with 74338 rows\n"
     ]
    }
   ],
   "source": [
    "# only take rows with speakers in the congress or senator files\n",
    "# only take rows with quotations that contain relevant words\n",
    "chunk_num = 1\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        \n",
    "        return chunk.loc[chunk.speaker.isin(names) & (chunk.quotation.str.contains('{}'.format(\"|\".join(words))))]\n",
    "            \n",
    "        \n",
    "\n",
    "with pd.read_json('data/quotes-2015.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe15 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe15 = pd.concat([dataframe15, processed_chunk])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "305bf8d1-941f-4666-a2c4-24f4fafd48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge quote data with speaker state data\n",
    "merged = pd.merge(dataframe15, politicians, on='speaker', how='inner')\n",
    "merged.to_csv(\"data/df15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecb8f4a8-9fb4-4cb1-bd06-859443dd854e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>2015-10-05-032377</td>\n",
       "      <td>`I believe in gay marriage. I'm not gon na cha...</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>[Q359442]</td>\n",
       "      <td>2015-10-05 17:25:03</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Bernie Sanders, 0.7745], [None, 0.1729], [Te...</td>\n",
       "      <td>[http://www.centralmaine.com/2015/10/05/in-rur...</td>\n",
       "      <td>E</td>\n",
       "      <td>Vermont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>2015-09-17-023875</td>\n",
       "      <td>For those without a prior registration as a gu...</td>\n",
       "      <td>Paul Mitchell</td>\n",
       "      <td>[Q20706988, Q27922634, Q28819608, Q30121955, Q...</td>\n",
       "      <td>2015-09-17 20:26:23</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Paul Mitchell, 0.8964], [None, 0.0908], [Jer...</td>\n",
       "      <td>[http://cafwd.org/blog/entry/automatic-registr...</td>\n",
       "      <td>E</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2015-05-22-068215</td>\n",
       "      <td>We can't have marriage equality in the United ...</td>\n",
       "      <td>Ted Cruz</td>\n",
       "      <td>[Q2036942]</td>\n",
       "      <td>2015-05-22 16:20:53</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Ted Cruz, 0.4141], [None, 0.392], [Heidi Cru...</td>\n",
       "      <td>[http://dailykos.com/story/2015/05/20/1386172/...</td>\n",
       "      <td>E</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2015-04-27-054572</td>\n",
       "      <td>Today's Democratic Party has become so radical...</td>\n",
       "      <td>Ted Cruz</td>\n",
       "      <td>[Q2036942]</td>\n",
       "      <td>2015-04-27 17:21:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Ted Cruz, 0.6226], [None, 0.3774]]</td>\n",
       "      <td>[http://www.thenewcivilrightsmovement.com/davi...</td>\n",
       "      <td>E</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2015-07-14-143991</td>\n",
       "      <td>Time is of the essence for the thousands of ac...</td>\n",
       "      <td>Jackie Speier</td>\n",
       "      <td>[Q218544]</td>\n",
       "      <td>2015-07-14 14:17:51</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Jackie Speier, 0.8209], [None, 0.1791]]</td>\n",
       "      <td>[http://washingtontimes.com/news/2015/jul/13/d...</td>\n",
       "      <td>E</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               quoteID                                          quotation  \\\n",
       "407  2015-10-05-032377  `I believe in gay marriage. I'm not gon na cha...   \n",
       "799  2015-09-17-023875  For those without a prior registration as a gu...   \n",
       "149  2015-05-22-068215  We can't have marriage equality in the United ...   \n",
       "189  2015-04-27-054572  Today's Democratic Party has become so radical...   \n",
       "288  2015-07-14-143991  Time is of the essence for the thousands of ac...   \n",
       "\n",
       "            speaker                                               qids  \\\n",
       "407  Bernie Sanders                                          [Q359442]   \n",
       "799   Paul Mitchell  [Q20706988, Q27922634, Q28819608, Q30121955, Q...   \n",
       "149        Ted Cruz                                         [Q2036942]   \n",
       "189        Ted Cruz                                         [Q2036942]   \n",
       "288   Jackie Speier                                          [Q218544]   \n",
       "\n",
       "                   date  numOccurrences  \\\n",
       "407 2015-10-05 17:25:03               1   \n",
       "799 2015-09-17 20:26:23               2   \n",
       "149 2015-05-22 16:20:53               1   \n",
       "189 2015-04-27 17:21:00               1   \n",
       "288 2015-07-14 14:17:51               2   \n",
       "\n",
       "                                                probas  \\\n",
       "407  [[Bernie Sanders, 0.7745], [None, 0.1729], [Te...   \n",
       "799  [[Paul Mitchell, 0.8964], [None, 0.0908], [Jer...   \n",
       "149  [[Ted Cruz, 0.4141], [None, 0.392], [Heidi Cru...   \n",
       "189               [[Ted Cruz, 0.6226], [None, 0.3774]]   \n",
       "288          [[Jackie Speier, 0.8209], [None, 0.1791]]   \n",
       "\n",
       "                                                  urls phase       state  \n",
       "407  [http://www.centralmaine.com/2015/10/05/in-rur...     E     Vermont  \n",
       "799  [http://cafwd.org/blog/entry/automatic-registr...     E    Michigan  \n",
       "149  [http://dailykos.com/story/2015/05/20/1386172/...     E       Texas  \n",
       "189  [http://www.thenewcivilrightsmovement.com/davi...     E       Texas  \n",
       "288  [http://washingtontimes.com/news/2015/jul/13/d...     E  California  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c72b39-9b95-4e7d-8815-5f5c47d69079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ad03aa9-8506-462c-aa3c-7867ac48f124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567           immediately eliminate gender pay inequity.\n",
       "541    There is no federal constitutional right to sa...\n",
       "535    reverse the executive orders the President has...\n",
       "354    And furthermore, what they mean by family valu...\n",
       "65     You have no entitlement to force that (gay) fl...\n",
       "461    The column says nothing about gays; it's about...\n",
       "596    slap in the face of the thousands of gay and l...\n",
       "651    Most Americans already believe that people in ...\n",
       "625    buy into those concepts of sexual orientation ...\n",
       "258    And so, I've often said we could of gotten aro...\n",
       "Name: quotation, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.quotation.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b9c4168-3ff7-456c-8017-607ffecef781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Texas                   198\n",
       "Vermont                 158\n",
       "Florida                  91\n",
       "California               62\n",
       "Kentucky                 51\n",
       "Illinois                 31\n",
       "South Carolina           24\n",
       "New York                 17\n",
       "Colorado                 14\n",
       "Wisconsin                13\n",
       "Washington               12\n",
       "Iowa                     12\n",
       "Ohio                     11\n",
       "Arizona                  11\n",
       "New Jersey               11\n",
       "Georgia                  11\n",
       "Hawaii                   10\n",
       "Kansas                    9\n",
       "Maine                     8\n",
       "Utah                      8\n",
       "Arkansas                  8\n",
       "Virginia                  7\n",
       "Louisiana                 6\n",
       "Oregon                    6\n",
       "Oklahoma                  5\n",
       "Massachusetts             5\n",
       "Tennessee                 5\n",
       "Alaska                    5\n",
       "Nevada                    4\n",
       "Maryland                  4\n",
       "Minnesota                 3\n",
       "Nebraska                  3\n",
       "Pennsylvania              3\n",
       "Michigan                  3\n",
       "Connecticut               3\n",
       "Delaware                  2\n",
       "Rhode Island              2\n",
       "District of Columbia      2\n",
       "New Hampshire             2\n",
       "Wyoming                   1\n",
       "Alabama                   1\n",
       "New Mexico                1\n",
       "North Dakota              1\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2fe7c-ae2f-4498-9f94-7bd0b232f7c3",
   "metadata": {},
   "source": [
    "#### can do the same for each year..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b4d28-57c3-49f8-886f-c5d64e4bd189",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_num = 1\n",
    "with pd.read_json('quotes-2016.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe16 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe16 = pd.concat([dataframe16, processed_chunk])\n",
    "\n",
    "        \n",
    "merged16 = pd.merge(dataframe16, politicians, on='speaker', how='inner')\n",
    "merged16.to_csv(\"data/df16.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565c639-b2a6-43bb-9bbf-16e0ba17699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_num = 1\n",
    "with pd.read_json('data/quotes-2017.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe17 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe17 = pd.concat([dataframe17, processed_chunk])\n",
    "        \n",
    "merged17 = pd.merge(dataframe17, politicians, on='speaker', how='inner')\n",
    "merged17.to_csv(\"data/df17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78555936-7a5f-435f-be87-1ddace806d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_num = 1\n",
    "with pd.read_json('data/quotes-2018.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe18 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe18 = pd.concat([dataframe18, processed_chunk])\n",
    "\n",
    "merged18 = pd.merge(dataframe18, politicians, on='speaker', how='inner')\n",
    "merged18.to_csv(\"data/df18.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4baaea-d18f-4197-938f-46b3fa15b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_num = 1\n",
    "with pd.read_json('data/quotes-2019.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe19 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe19 = pd.concat([dataframe19, processed_chunk])\n",
    "\n",
    "merged19 = pd.merge(dataframe19, politicians, on='speaker', how='inner')\n",
    "merged19.to_csv(\"data/df19.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa8643-6528-44bf-8fae-71c5e2317ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_num = 1\n",
    "with pd.read_json('data/quotes-2020.json.bz2', lines=True, compression='bz2', chunksize=100000) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if (chunk_num == 1):\n",
    "            dataframe20 = processed_chunk\n",
    "            chunk_num += 1\n",
    "        else:\n",
    "            dataframe20 = pd.concat([dataframe20, processed_chunk])\n",
    "            \n",
    "merged20 = pd.merge(dataframe20, politicians, on='speaker', how='inner')\n",
    "merged20.to_csv(\"data/df20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b892d-795d-443f-9dcb-57773d244513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b0c43-6e0c-4677-bb86-b522724da0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
